---
title: "ADA Final Project"
author: "Abbie Tolon"
date: "March 31, 2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Set-up

First, I installed all of the necessary packages and loaded them to my library. Next, I retrieved the csv file from my gitbuh repository. The dataset contained 6982 total observations and 21 different variables. 

```{r echo=TRUE, results='hide'}
##installing/loading the latest installr package:
     #install.packages("installr"); library(installr) 

##Update version of R
     #updateR() #updating R.

##Clear the global environment 
     remove(list = ls())

##Install packages to be used for this analysis
     #install.packages(c("tidyverse", "ResourceSelection", "odds.n.ends", "lmtest", "car", "pacman"))

#Load all necessary packages
pacman::p_load(tidyverse, ResourceSelection, odds.n.ends, lmtest, car)

#tidyverse = For various packages
#ResourceSelection = For Hosmer Lemeshow test
#odds.n.ends = For CIs, ORs, and sensitivity/specificity
#lmtest = For testing and comparing linear regression models
#car = For compareCoefs function

#Retrieve Data from Github -----------------------------------------------------------------------------
#Link for full sample
data <-read.csv("https://raw.githubusercontent.com/artolon/ADA2/master/ADA/ADA%20Final%20Project/ADA_Final_Key_Variables.csv")
```

### Data Cleaning

**Study population** 	
The study will include all incarcerated adults, above the age of 18 in the data set

**Variables  of Interest**
I will use the following variables in my analysis:  
* *Dependent variable:* Ever injected drugs (yes/no)  
* *Independent variables:* Ever had a disease (yes/no), which will be aggregated from...TB test result (positive/negative); HIV test result (positive/negative); AIDS test results (positive/negative); ever had hepatitis; ever had STD  
* *Confounding variables:* Sex, Race (dichotomized in data set to be black vs. no black), Age

```{r echo=TRUE, results='hide'}
#Filter the data set to only inlcude people 18 and older 
data <- data[data$Age > 17,]
data #after filtering, there are now 6601 observations and 21 variables

#Get names of the data
names(data)

#Select only the columns that we want to work with
data <- select(data, c(Age,V4...Sex,V44...Black.1,V1794...Ever.injected.Drugs,V1899...TB.Test.result,V1911...Result.of.last.HIV.test,V1913...Result.of.AIDS.test,V1993...Ever.had.hepatitus,V1996...Ever.Had.STD))

#View Updated Data
data

#Change colunm names to be easier to read
names(data) <- c("Age", "Sex", "Black", "IDU", "TB", "HIV", "AIDS", "Hep", "STD")
view(data)

#Check class of Age variable
class(data$Age) #is integer 

#Recode all variables so that there are only 1s and 0s, rather than 1s and 2s
#For sex variable -> 1=Male, 0=Female
#For race variable -> 1=Black, 0=Non-black
#For IDU, TB, HIV, and AIDS variables -> 1=positive test result, 0=negative test result
#For Hep and STD variables -> 1=Yes, 0=No
data <- data %>%
  mutate(Sex=ifelse(Sex==1,1, ifelse(Sex==2,0,NA)),
         Black=ifelse(Black==1,1, ifelse(Black==2,0,NA)),
         IDU=ifelse(IDU==1,1, ifelse(IDU==2,0,NA)),
         TB=ifelse(TB==1,1, ifelse(TB==2,0,NA)),
         HIV=ifelse(HIV==1,1, ifelse(HIV==2,0,NA)),
         AIDS=ifelse(AIDS==1,1, ifelse(AIDS==2,0,NA)),
         Hep=ifelse(Hep==1,1, ifelse(Hep==2,0,NA)),
         STD=ifelse(STD==1,1, ifelse(STD==2,0,NA)))
```

### Inspecting Variables

**Demographic Variables** - *Age, Race (Black/Not Black), Sex (male/female)*
```{r echo=TRUE, results='hide'}
#AGE------------------------------------------------------------------------------------
#Find the median, min, and max age
quantile(data$Age, na.rm = TRUE) #median=31; min=18; max=82

#Find avaerage age 
mean(data$Age, na.rm = TRUE) #average age = 32

#count missing variables 
sum(is.na(data$Age)) #only 3 values are missing 

#create a new variable that recodes age into a categorical variable 
data <- data %>%
  mutate(AgeCat=ifelse(Age>17 & Age<=34, 1, ifelse(
    Age>34 & Age <=50, 2, ifelse(Age>50 & Age<=99, 3, NA))))

#check new variable
table(data$AgeCat, data$Age) #table looks good!

#Check class of new AgeCat variable
class(data$AgeCat) #is numeric
sum(is.na(data$AgeCat)) #3 missing; 6598 observations

#Look at the breakdown for Age cat
sum(data$AgeCat == 1, na.rm = TRUE) #3968 people between 18 and 34
sum(data$AgeCat == 2, na.rm = TRUE) #2366 people between 35 and 50
sum(data$AgeCat == 3, na.rm = TRUE) #264 people are older than 50

(3968/6598)*100 #60.14% people between 18 and 34
(2366/6598)*100 #35.86% people between 35 and 50
(264/6598)*100 #4.00% people are older than 50



#RACE-------------------------------------------------------------------------------------
#Check class of race variable
class(data$Black) #is numeric
sum(is.na(data$Black)) #3 missing; 6598 observations

#Identify the number of people in the sample who identify as Black
sum(data$Black==1, na.rm = TRUE) #2770 people identify as Black

(2770/6598)*100 #41.98 identified as "Black"
100-((2770/6598)*100) #58.02 identified as "Nonblack"



#SEX--------------------------------------------------------------------------------------
#Check class of sex variable
class(data$Sex) #is numeric
sum(is.na(data$Sex)) #3 missing; 6598 observations

#Identify the number of people in the sample who are male
sum(data$Sex==1, na.rm = TRUE) #4635 are male

(4635/6598)*100 #70.25 identified as male
100-((4635/6598)*100) #29.75 identified as female
```

**Independent Variable** - *Aggregated from...TB test result (positive/negative); HIV test result (positive/negative); AIDS test results (positive/negative); ever had hepatitis (yes/no); ever had STD (yes/no)*
```{r echo=TRUE, results='hide'}
#TB--------------------------------------------------------------------------------------
#Check class of TB variable
class(data$TB) #is numeric
sum(is.na(data$TB)) #2755 missing; 3846 observations

#Identify the number of people in the sample who have ever tested positive for TB
sum(data$TB==1, na.rm = TRUE) #113 have ever tested positive for TB

(113/3846)*100 #2.94% have ever tested positive for TB
100-((113/3846)*100) #97.06% have never tested positive for TB



#HIV--------------------------------------------------------------------------------------
#Check class of HIV variable
class(data$HIV) #is numeric
sum(is.na(data$HIV)) #5495 missing; 1106 observations

#Identify the number of people in the sample who have ever tested positive for HIV
sum(data$HIV==1, na.rm = TRUE) #37 have ever tested positive for HIV

(37/1106)*100 #3.35% have ever tested positive for HIV
100-((37/1006)*100) #96.32% have never tested positive for HIV



#AIDS--------------------------------------------------------------------------------------
#Check class of AIDS variable
class(data$AIDS) #is numeric
sum(is.na(data$AIDS)) #3633 missing; 2968 observations

#Identify the number of people in the sample who have ever tested positive for AIDS
sum(data$AIDS==1, na.rm = TRUE) #32 have ever tested positive for AIDS

(32/2968)*100 #1.08% have ever tested positive for AIDS
100-((32/2968)*100) #98.92% have never tested positive for AIDS



#Hep--------------------------------------------------------------------------------------
#Check class of Hepatitis variable
class(data$Hep) #is numeric
sum(is.na(data$Hep)) #115 missing; 6486 observations

#Identify the number of people in the sample who have ever had hepatitis
sum(data$Hep==1, na.rm = TRUE) #412 have ever had hepatitis

(412/6486)*100 #6.35% have ever had hepatits
100-((412/6486)*100) #93.65% have never had hepatitis



#STD--------------------------------------------------------------------------------------
#Check class of STD variable
class(data$STD) #is numeric
sum(is.na(data$STD)) #112 missing; 6489 observations

#Identify the number of people in the sample who have ever had a STD
sum(data$STD==1, na.rm = TRUE) #873 have ever had a STD

(873/6489)*100 #13.45% have ever had a STD
100-((873/6489)*100) #86.55% have never had a STD
```

Combine all of the disease variables above into one aggregated dichotomous variable (Infectious Disease yes/no)

```{r echo=TRUE, results='hide'}
data <- data %>%
  mutate(Disease = ifelse(TB==1|HIV==1|AIDS==1|Hep==1|STD==1, 1, 0))

#Zeros did not recode properly; fix here
data$Disease[is.na(data$Disease)] <- 0

#See how many had one of the diseases
sum(data$Disease==1, na.rm = TRUE) #1285 have had TB, HIV, Hepatitis, or a STD

(1285/6601)*100 #19.47 have ever had TB, HIV, Hepatitis, or a STD
100-((1285/6601)*100) #80.53 have never had TB, HIV, Hepatitis, or a STD
```


**Dependent Variable** -	*Ever injected drugs (yes/no)*
```{r echo=TRUE, results='hide'}
#Check class of IDU variable
class(data$IDU) #is numeric
sum(is.na(data$IDU)) #1246 missing; 5355 observations

#Identify the number of people in the sample who have injected drugs
sum(data$IDU==1, na.rm = TRUE) #1065 have injected drugs

(1065/5355)*100 #19.89% have injected drugs before
100-((1065/5355)*100) #80.11% have never injected drugs
```

### Data Analysis

Creating a dataset with complete cases for further analysis

**Complete Cases**
```{r echo=TRUE}
#drop all NAs from the IDU variable before running analysis
data_cc <- data %>%
  select(Age, Sex, Black, IDU, Disease, AgeCat) %>%
  drop_na()

#summarizing the data
summary(data_cc) #5355 observations and 5 variables 
```

**Assumptions**  
- Dependent variable is dichotomous (Have you ever injected drugs? yes/no)  
- There are multiple independent variables, and they all vary  
- There is independence of observations (individual survey respondents)  
- All categories for dichotomous dependent and independent variables are exhaustive and mutually exclusive  
- There are more than 50 cases per independent variable  
- There is no multicollinearity (see below)  
- All overly influential values were removed (see below)

**Check Linearity Assumption for Age Variable**
```{r echo=TRUE}
#Check for linearity of age variable with the box tidwell technique

#linearity
logAge <- data_cc$Age*log(data_cc$Age)#create term to test linearity

#Box Tidwell to test assumption of linearity 
BoxTid_age <- glm(IDU ~ Age + logAge, data=data_cc, family="binomial") 
summary(BoxTid_age) #The term for linearity IS significant; therefore we violate the assumption and will not include age in the model
```

**Run the general linear model, without confounders included (unadjusted model)**
```{r echo=TRUE}
#Run the unadjusted model with disease as predictor 
model_unadjust <- glm(IDU ~ Disease, data=data_cc, family="binomial")

#Check results of the model
odds.n.ends(model_unadjust) #significant p-value for Disease; OR = 2.88, 95% CI (2.49, 3.33)

#Run the Hosmer Lemeshow test to show Goodness of Fit
hoslem.test(data_cc$IDU, fitted(model_unadjust)) #Non-significant p-value, showing acceptable fit
```

*Model prediction accuracy*
- The accuracy of the above model is (0+4290)/5355=80.11%. It predicts all 0's correctly (100% correct) and all 1's incorrect (0% correct)  
- In otherwords, this model does very well at predicting non-cases (non injection drug users), but does extremely poorly at predicting cases (injection drug users)
- This is perhaps unsurprising, as logistic regression does not work well as a classifier when the prevalence of the outcome is low. In this case, the prevalence is approximately 20%

**Run the general linear model, with confounders included (adjusted model)**
```{r echo=TRUE}
#Run the adjusted model (Include sex and race)
model_adjust <- glm(IDU ~ Disease + Sex + Black, data=data_cc, family="binomial")

#Check results of the model
odds.n.ends(model_adjust) #All values are significant; Disease OR = 3.69, 95% CI (3.14, 4.34)

#Run the Hosmer Lemeshow test to show Goodness of Fit
hoslem.test(data_cc$IDU, fitted(model_adjust)) #nonsignificant hosmer lemeshow; accpetable fit
```
*Model prediction accuracy*
- The accuracy of the above model is (153+4163)/5355=80.60%. It predicts most 0's correctly (97.04% correct) and most 1's incorrect (14.37% correct)
- This model is slightly better than the previous model. However, we still want to try to improve

**Check for Influential Observations**
```{r echo=TRUE}
#Cook's D plot
plot(model_adjust, which=4, id.n=5, col="red", cex.id=0.60) 
```
```{r echo=TRUE, results='hide'}
#identify observations with a Cook's D greater than 0.003 
y<-as.data.frame(cooks.distance(model_adjust))
colnames(y)[1]<-"CD"
y$obs_no<-rownames(y)
z<-y[which(y$CD>0.002),]
z$obs_no

#The following observations are influential:
#393,425,486,576,587,641,705,737,802,1049,1274,1394,1465,1488,1494,1547,1580,1677,1706,1739,1741,1762,1781,1793,1818,1822,1866,1880,1881,1923,1925

#Remove the influential observations from adjusted model
model_adjust2 <- update(model_adjust,subset=c(-393,-425,-486,-576,-587,-641,-705,-737,-802,-1049,-1274,-1394,-1465,-1488,-1494,-1547,-1580,-1677,-1706,-1739,-1741,-1762,-1781,-1793,-1818,-1822,-1866,-1880,-1881,-1923,-1925))

#compare coefficients between models with and without influential observations
compareCoefs(model_adjust, model_adjust2) #removing the observations made very little difference on the coefficients, but the effect that disease has is slightly stronger 

#Remove influential observations from unadjusted model
model_unadjust2 <- update(model_unadjust,subset=c(-393,-425,-486,-576,-587,-641,-705,-737,-802,-1049,-1274,-1394,-1465,-1488,-1494,-1547,-1580,-1677,-1706,-1739,-1741,-1762,-1781,-1793,-1818,-1822,-1866,-1880,-1881,-1923,-1925))

#compare coefficients between models with and without influential observations
compareCoefs(model_unadjust, model_unadjust2) #removing the observations made very little difference on the coefficients, but the effect that disease has is slightly stronger 
```

**Check Multicollinearity**
```{r echo=TRUE}
#Variance Inflation Factors
vif(model_adjust2)

#The VIF for each variable is less than 2.0, so we are not concerned with multicollinearity 
```

**Check the log liklihood between the adjusted and unadjusted models**
Use log liklihood to see how much unexplained information there is after the model has been fitted. The further a predicted value is from the actual value, the more an observation contributes to the LL
```{r echo=TRUE}
#Log Likelihood for full models
logLik(model_unadjust2) #LL = -2556.24
logLik(model_adjust2) #LL = -2333.11

#compare models using LR test
lmtest::lrtest(model_unadjust2, model_adjust2) #The adjusted model accounts for more of the variation and is statistically significantly better than the unadjusted model 

anova(model_unadjust2, model_adjust2,test="Chisq") #adjusted model is statistically significantly better
```

### See if the model improves after balancing the data set

Because the proportion of cases are low, we are going to balance the data set to see if this allows us to better predict 0s vs. 1s

```{r}
#First remove the influential observations from the full data set
data_remove_influence = data_cc[-c(393,425,486,576,587,641,705,737,802,1049,1274,1394,1465,1488,1494,1547,1580,1677,1706,1739,1741,1762,1781,1793,1818,1822,1866,1880,1881,1923,1925),]

#Next, create a data set that only has cases (IDU=1)
data_one<-data_remove_influence[which(data_remove_influence$IDU==1),] #1057 observations

#Next create a data set that only has non cases (IDU=0)
#set the seed so that we always get the same results
set.seed(1)
data_zero<-sample_n(data_remove_influence[which(data_remove_influence$IDU==0),], size=1057,) #1057 observations 

#Now, combine the two newly created data sets into 1, so that the sample is balanced 
data_balance<- rbind(data_zero,data_one) #Combine these datasets by row 
#We now have 2114 observations, which makes sense! 
```

### Run model with new data set

```{r}
#Logistic regression model with a balanced data set
model_balance <-  glm(IDU ~ Disease + Sex + Black, data=data_balance, family="binomial")
odds.n.ends(model_balance)
```

Interestingly, this model drastically changes the sensitivity and specificity. Overall we get (905+489/2114 = 65.94%) correct. This means that this model (overall) does more poorly than the unbalanced model (overall). However, the sensitivity drastically improved (while also lowering specificity quite a bit). This means we correctly predict our cases 85.62% of the time, but we predict our non-cases only 46.26% of the time. 

### Compute ROC curve to get AUC

**Receiver Operating Characteristic Curve (ROC curve)**
```{r}
#First, compute the predicted probabilties from the model and actual values of IDU from the balanced data set that was created above
predict <- predict(model_balance, newdata=data_balance, type = "response")

#Create terms for both ROC and performance 
ROC=prediction(as.numeric(predict), as.numeric(data_balance$IDU)) 
p=performance(ROC,"tpr", "fpr")

#Plot the results
plot(p, colorsize=T, color="red", print.cutoffs.at=seq(0,1,0.2))
abline(a=0, b= 1)

#Get the threshold for where we have highest sensitivity and specificity 
#Look visually at where threshold would be
thresh <- performance(ROC, "sens", "spec")
plot(thresh)

#See value for threshold 
threshval<-thresh@alpha.values[[1]][which.max(thresh@x.values[[1]]+thresh@y.values[[1]])]
threshval #value is 0.52
```

**Area Under the Curve (AUC)**
```{r}
#Calculate the AUC
auc = performance(ROC, measure = "auc")
auc@y.values

#Value is 0.736
```

The value of the AUC is approximately 0.736, indicating that the balanced model does okay at predicting people who have used injection drugs. 

### Interpretation and conclusions (Discussion)


### Package Citations

Create citations for R, RStudio, and all packages used
```{r echo=TRUE}
#Citation for R
#citation()

#Citation for R Studio
#RStudio.Version()

#For packages
#citation(package = "tidyverse")
#citation(package = "ResourceSelection")
#citation(package = "odds.n.ends")
#citation(package = "lmtest")
#citation(package = "car")
```

